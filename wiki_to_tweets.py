#!/usr/bin/env python3
"""
Wiki to Twitter Posts Converter
å°†æ—¥å‘å‚46 Fandom Wiki HTMLé¡µé¢è½¬æ¢ä¸ºTwitteré£Žæ ¼çš„åŠ¨æ€
"""

import os
import json
import re
from pathlib import Path
from bs4 import BeautifulSoup
from dataclasses import dataclass, asdict
from typing import Optional, List
import subprocess


@dataclass
class MemberProfile:
    """æˆå‘˜èµ„æ–™æ•°æ®ç±»"""
    name_en: str
    name_jp: str = ""
    nickname: str = ""
    birthday: str = ""
    birthplace: str = ""
    blood_type: str = ""
    zodiac: str = ""
    height: str = ""
    occupation: str = ""
    years_active: str = ""
    agency: str = ""
    generation: str = ""
    group: str = ""
    bio: str = ""  # ç®€ä»‹/æè¿°


def parse_wiki_html(html_path: str) -> MemberProfile:
    """è§£æžWiki HTMLæ–‡ä»¶ï¼Œæå–æˆå‘˜èµ„æ–™"""
    with open(html_path, 'r', encoding='utf-8') as f:
        content = f.read()

    soup = BeautifulSoup(content, 'html.parser')

    # èŽ·å–è‹±æ–‡åï¼ˆä»Žæ ‡é¢˜ï¼‰
    title_elem = soup.find('h1', class_='page-header__title')
    name_en = ""
    if title_elem:
        span = title_elem.find('span', class_='mw-page-title-main')
        if span:
            name_en = span.get_text(strip=True)
        else:
            name_en = title_elem.get_text(strip=True)

    # ä»Žinfoboxæå–æ•°æ®
    infobox = soup.find('aside', class_='portable-infobox')

    profile = MemberProfile(name_en=name_en)

    if not infobox:
        # å°è¯•ä»Žmeta descriptionèŽ·å–åŸºæœ¬ä¿¡æ¯
        meta_desc = soup.find('meta', {'name': 'description'})
        if meta_desc:
            profile.bio = meta_desc.get('content', '')
        return profile

    # èŽ·å–æ—¥æ–‡å
    title_h2 = infobox.find('h2', class_='pi-title')
    if title_h2:
        ruby_tags = title_h2.find_all('ruby')
        if ruby_tags:
            jp_name_parts = []
            for ruby in ruby_tags:
                rb = ruby.find('rb')
                if rb:
                    jp_name_parts.append(rb.get_text(strip=True))
            profile.name_jp = ''.join(jp_name_parts)

    # æå–å„é¡¹æ•°æ®
    data_items = infobox.find_all('div', class_='pi-data')

    for item in data_items:
        label = item.find('h3', class_='pi-data-label')
        value = item.find('div', class_='pi-data-value')

        if not label or not value:
            continue

        label_text = label.get_text(strip=True).lower()
        value_text = value.get_text(strip=True)

        # æ¸…ç†value_textä¸­çš„å¤šä½™å†…å®¹
        value_text = re.sub(r'\s+', ' ', value_text)

        if 'nickname' in label_text:
            profile.nickname = value_text
        elif 'born' in label_text or 'birthday' in label_text:
            profile.birthday = value_text
        elif 'birthplace' in label_text:
            profile.birthplace = value_text
        elif 'blood' in label_text:
            profile.blood_type = value_text
        elif 'zodiac' in label_text:
            profile.zodiac = value_text
        elif 'height' in label_text:
            profile.height = value_text
        elif 'occupation' in label_text:
            profile.occupation = value_text
        elif 'years active' in label_text or 'active' in label_text:
            profile.years_active = value_text
        elif 'agency' in label_text:
            profile.agency = value_text
        elif 'generation' in label_text:
            profile.generation = value_text

    # èŽ·å–ç®€ä»‹ï¼ˆä»Žmeta descriptionæˆ–æ­£æ–‡ï¼‰
    meta_desc = soup.find('meta', {'name': 'description'})
    if meta_desc:
        profile.bio = meta_desc.get('content', '')

    # åˆ¤æ–­æ‰€å±žå›¢ä½“
    if 'hinatazaka' in html_path.lower() or 'hinata' in str(infobox).lower():
        profile.group = "Hinatazaka46"
    elif 'nogizaka' in html_path.lower():
        profile.group = "Nogizaka46"
    elif 'sakurazaka' in html_path.lower():
        profile.group = "Sakurazaka46"

    return profile


def call_gemini_via_node(prompt: str) -> Optional[str]:
    """é€šè¿‡ Node.js ai-sdk è°ƒç”¨ Gemini API"""
    ai_script_dir = Path('/Users/jason/Downloads/workflow/llm-api/ai-script')

    if not ai_script_dir.exists():
        return None

    try:
        result = subprocess.run(
            ['node', 'index.js', prompt],
            capture_output=True,
            text=True,
            timeout=120,
            cwd=str(ai_script_dir)
        )

        if result.returncode == 0 and result.stdout.strip():
            return result.stdout.strip()
        else:
            if result.stderr:
                print(f"Node.js é”™è¯¯: {result.stderr[:200]}")
            return None

    except subprocess.TimeoutExpired:
        print("Gemini è°ƒç”¨è¶…æ—¶")
        return None
    except FileNotFoundError:
        print("æœªæ‰¾åˆ° Node.js")
        return None
    except Exception as e:
        print(f"Gemini è°ƒç”¨å¤±è´¥: {e}")
        return None


def call_gemini_api(prompt: str, credential_path: str) -> Optional[str]:
    """è°ƒç”¨ Google Vertex AI (Gemini) API - REST æ–¹å¼"""
    try:
        from google.oauth2 import service_account
        from google.auth.transport.requests import Request
        import requests

        # è¯»å–å‡­è¯
        with open(credential_path, 'r') as f:
            creds_data = json.load(f)

        credentials = service_account.Credentials.from_service_account_file(
            credential_path,
            scopes=['https://www.googleapis.com/auth/cloud-platform']
        )

        # èŽ·å–è®¿é—®ä»¤ç‰Œ
        credentials.refresh(Request())
        access_token = credentials.token

        project_id = creds_data['project_id']
        location = 'asia-northeast1'

        # å°è¯•å¤šä¸ªæ¨¡åž‹ç‰ˆæœ¬
        models_to_try = [
            'gemini-1.5-flash',
            'gemini-1.5-pro',
            'gemini-pro'
        ]

        for model in models_to_try:
            url = f"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model}:generateContent"

            headers = {
                'Authorization': f'Bearer {access_token}',
                'Content-Type': 'application/json'
            }

            data = {
                "contents": [
                    {
                        "role": "user",
                        "parts": [{"text": prompt}]
                    }
                ],
                "generationConfig": {
                    "maxOutputTokens": 1024,
                    "temperature": 0.8
                }
            }

            try:
                response = requests.post(url, headers=headers, json=data, timeout=60)
                if response.status_code == 200:
                    result = response.json()
                    return result['candidates'][0]['content']['parts'][0]['text']
            except:
                continue

        return None

    except ImportError as e:
        return None
    except Exception as e:
        print(f"Gemini REST APIå¤±è´¥: {e}")
        return None


def generate_tweets_with_ai(profile: MemberProfile, api_key: Optional[str] = None) -> List[str]:
    """ä½¿ç”¨AIç”ŸæˆTwitteré£Žæ ¼çš„åŠ¨æ€"""

    profile_dict = asdict(profile)
    profile_json = json.dumps(profile_dict, ensure_ascii=False, indent=2)

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¶åƒç²‰ä¸è´¦å·è¿è¥è€…ã€‚æ ¹æ®ä»¥ä¸‹æ—¥å‘å‚46æˆå‘˜çš„èµ„æ–™ï¼Œç”Ÿæˆ8-10æ¡Twitteré£Žæ ¼çš„åŠ¨æ€ï¼ˆæŽ¨æ–‡ï¼‰ã€‚

æˆå‘˜èµ„æ–™ï¼š
{profile_json}

è¦æ±‚ï¼š
1. æ¯æ¡æŽ¨æ–‡ä¸è¶…è¿‡280å­—ç¬¦ï¼ˆä¸­æ–‡çº¦140å­—ï¼‰
2. å¯ä»¥åŒ…å«emojiï¼Œä½†ä¸è¦è¿‡å¤š
3. é£Žæ ¼è¦åƒçœŸå®žçš„ç²‰ä¸åˆ†äº«ï¼Œæœ‰çƒ­æƒ…ä½†ä¸å¤¸å¼ 
4. æŽ¨æ–‡ç±»åž‹è¦å¤šæ ·åŒ–ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºŽï¼š
   - æˆå‘˜åŸºæœ¬ä»‹ç»
   - ç”Ÿæ—¥ç¥ç¦æ¨¡æ¿ï¼ˆåªåœ¨ç”Ÿæ—¥å½“å¤©ç”Ÿæˆï¼‰
   - æ˜µç§°/å¤–å·è¶£äº‹
   - èº«é«˜/æ˜Ÿåº§/è¡€åž‹ç­‰å†·çŸ¥è¯†
   - å‡ºèº«åœ°ç›¸å…³
   - åŠ å…¥å›¢ä½“çš„ç»åŽ†
   - é¼“åŠ±åº”æ´çš„å†…å®¹
   - æ—¥å¸¸å®‰åˆ©æŽ¨è
   - å’Œå…¶ä»–æˆå‘˜çš„å…³ç³»
   - ç»æŠ€ã€ç»æ‹›
   - çˆ±å¥½
   - å¦‚æžœæœ‰åŠ¨ç‰©å¡‘ï¼Œå¯ä»¥å‘æŒ¥ä¸€ä¸‹
   - å¦‚æžœæœ‰åº”æ´è‰²ï¼Œå¯ä»¥è¯´æ˜Ž
5. ä½¿ç”¨æ—¥æ–‡æ’°å†™
6. æ¯æ¡æŽ¨æ–‡ç”¨ "---" åˆ†éš”
7. å¿…é¡»ç”Ÿæˆè‡³å°‘8æ¡ä¸åŒè§’åº¦çš„æŽ¨æ–‡
8. ä¸è¦åŠ hashtag

è¯·ç›´æŽ¥è¾“å‡ºæŽ¨æ–‡å†…å®¹ï¼Œä¸è¦è¾“å‡ºå…¶ä»–è§£é‡Šã€‚"""

    # ä¼˜å…ˆä½¿ç”¨ Node.js ai-sdk è°ƒç”¨ Geminiï¼ˆæœ€å¯é ï¼‰
    ai_script_dir = Path('/Users/jason/Downloads/workflow/llm-api/ai-script')
    if ai_script_dir.exists():
        print("  ä½¿ç”¨ Gemini API (Node.js)")
        response_text = call_gemini_via_node(prompt)
        if response_text:
            tweets = [t.strip() for t in response_text.split('---') if t.strip()]
            if tweets:
                return tweets

    # å¤‡é€‰ï¼šä½¿ç”¨ REST API è°ƒç”¨ Gemini
    credential_paths = [
        Path(__file__).parent / 'credential.json',
        Path('/Users/jason/Downloads/workflow/llm-api/ai-script/credential.json'),
        Path.home() / '.config' / 'gcloud' / 'application_default_credentials.json'
    ]

    for cred_path in credential_paths:
        if cred_path.exists():
            print(f"  ä½¿ç”¨ Gemini REST API ({cred_path.name})")
            response_text = call_gemini_api(prompt, str(cred_path))
            if response_text:
                tweets = [t.strip() for t in response_text.split('---') if t.strip()]
                if tweets:
                    return tweets
            break

    # å°è¯•ä½¿ç”¨Claude APIï¼ˆé€šè¿‡çŽ¯å¢ƒå˜é‡ï¼‰
    if api_key or os.environ.get('ANTHROPIC_API_KEY'):
        try:
            import anthropic
            client = anthropic.Anthropic(api_key=api_key) if api_key else anthropic.Anthropic()

            print("  ä½¿ç”¨ Claude API")
            message = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=1024,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )

            response_text = message.content[0].text
            tweets = [t.strip() for t in response_text.split('---') if t.strip()]
            return tweets
        except ImportError:
            pass
        except Exception as e:
            print(f"Claude APIè°ƒç”¨å¤±è´¥: {e}")

    # å°è¯•ä½¿ç”¨OpenAI API
    if os.environ.get('OPENAI_API_KEY'):
        try:
            import openai
            client = openai.OpenAI()

            print("  ä½¿ç”¨ OpenAI API")
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1024
            )

            response_text = response.choices[0].message.content
            tweets = [t.strip() for t in response_text.split('---') if t.strip()]
            return tweets
        except ImportError:
            pass
        except Exception as e:
            print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")

    # å°è¯•ä½¿ç”¨æœ¬åœ° Ollama
    try:
        print("  ä½¿ç”¨ Ollama")
        result = subprocess.run(
            ['ollama', 'run', 'llama3.2', prompt],
            capture_output=True,
            text=True,
            timeout=120
        )
        if result.returncode == 0:
            response_text = result.stdout
            tweets = [t.strip() for t in response_text.split('---') if t.strip()]
            if tweets:
                return tweets
    except FileNotFoundError:
        pass
    except subprocess.TimeoutExpired:
        print("Ollama è¶…æ—¶")
    except Exception as e:
        print(f"Ollamaè°ƒç”¨å¤±è´¥: {e}")

    # å›žé€€ï¼šä½¿ç”¨æ¨¡æ¿ç”ŸæˆåŸºç¡€æŽ¨æ–‡
    print("  ä½¿ç”¨æ¨¡æ¿æ¨¡å¼")
    return generate_template_tweets(profile)


def generate_template_tweets(profile: MemberProfile) -> List[str]:
    """ä½¿ç”¨æ¨¡æ¿ç”ŸæˆåŸºç¡€æŽ¨æ–‡ï¼ˆæ— AIæ—¶çš„å›žé€€æ–¹æ¡ˆï¼‰"""
    tweets = []

    name = profile.name_jp if profile.name_jp else profile.name_en

    # ä»‹ç»æŽ¨æ–‡
    intro = f"ã€æˆå‘˜ä»‹ç»ã€‘{name}"
    if profile.group:
        intro += f" æ˜¯ {profile.group} çš„æˆå‘˜"
    if profile.generation:
        intro += f"ï¼ˆ{profile.generation}ï¼‰"
    intro += " âœ¨"
    tweets.append(intro)

    # åŸºæœ¬ä¿¡æ¯
    if profile.birthday:
        tweets.append(f"ðŸŽ‚ {name} çš„ç”Ÿæ—¥æ˜¯ {profile.birthday}ï¼è®°å¾—ä¸ºå¥¹åº†ç¥å“¦ï½ž")

    if profile.birthplace:
        tweets.append(f"ðŸ“ {name} æ¥è‡ª{profile.birthplace}ï¼Œæ˜¯ä¸æ˜¯å¾ˆæƒ³åŽ»å¥¹çš„å®¶ä¹¡çœ‹çœ‹å‘¢ï¼Ÿ")

    if profile.nickname:
        tweets.append(f"ðŸ’« ä½ çŸ¥é“å—ï¼Ÿ{name} çš„æ˜µç§°æ˜¯ã€Œ{profile.nickname}ã€ï¼Œå¾ˆå¯çˆ±å§ï¼")

    if profile.height:
        tweets.append(f"ðŸ“ {name} èº«é«˜ {profile.height}ï¼Œåœ¨èˆžå°ä¸ŠçœŸçš„å¾ˆè€€çœ¼ï¼")

    if profile.zodiac and profile.blood_type:
        tweets.append(f"â­ {name} æ˜¯{profile.zodiac}åº§ï¼Œè¡€åž‹{profile.blood_type}åž‹ï½žä½ å’Œå¥¹åŒ¹é…å—ï¼Ÿ")

    return tweets[:5]  # æœ€å¤šè¿”å›ž5æ¡


def process_wiki_folder(wiki_folder: str, output_file: str = "tweets_output.json"):
    """å¤„ç†æ•´ä¸ªWikiæ–‡ä»¶å¤¹ï¼Œç”ŸæˆæŽ¨æ–‡"""

    wiki_path = Path(wiki_folder)
    html_files = list(wiki_path.rglob("*.html"))

    if not html_files:
        print(f"æœªæ‰¾åˆ°HTMLæ–‡ä»¶: {wiki_folder}")
        return

    print(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")

    all_results = []
    tweet_id = 1

    for html_file in html_files:
        print(f"\nå¤„ç†: {html_file.name}")

        try:
            profile = parse_wiki_html(str(html_file))

            if not profile.name_en:
                print(f"  è·³è¿‡ï¼ˆæ— æ³•æå–åå­—ï¼‰")
                continue

            print(f"  æˆå‘˜: {profile.name_en} ({profile.name_jp})")

            tweets = generate_tweets_with_ai(profile)

            # ä½¿ç”¨æ‰å¹³æ ¼å¼ï¼šæ¯æ¡æŽ¨æ–‡ä¸€ä¸ªå¯¹è±¡
            for tweet in tweets:
                all_results.append({
                    "id": tweet_id,
                    "ip": profile.group or "Hinatazaka46",
                    "content": tweet
                })
                tweet_id += 1

            print(f"  ç”Ÿæˆäº† {len(tweets)} æ¡æŽ¨æ–‡")

        except Exception as e:
            print(f"  é”™è¯¯: {e}")

    # ä¿å­˜ç»“æžœ
    output_path = Path(wiki_folder) / output_file
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"\nå®Œæˆï¼ç»“æžœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"å…±å¤„ç† {len(all_results)} ä¸ªæˆå‘˜")

    return all_results


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description='å°†Fandom Wiki HTMLè½¬æ¢ä¸ºTwitteråŠ¨æ€',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python wiki_to_tweets.py                    # å¤„ç†å½“å‰ç›®å½•
  python wiki_to_tweets.py ./info             # å¤„ç†æŒ‡å®šç›®å½•
  python wiki_to_tweets.py -o my_tweets.json  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶

çŽ¯å¢ƒå˜é‡:
  ANTHROPIC_API_KEY  - Claude APIå¯†é’¥ï¼ˆæŽ¨èï¼‰
  OPENAI_API_KEY     - OpenAI APIå¯†é’¥

å¦‚æžœæ²¡æœ‰è®¾ç½®APIå¯†é’¥ï¼Œå°†å°è¯•ä½¿ç”¨æœ¬åœ°Ollamaï¼Œæˆ–å›žé€€åˆ°æ¨¡æ¿ç”Ÿæˆã€‚
        """
    )

    parser.add_argument(
        'wiki_folder',
        nargs='?',
        default='.',
        help='Wiki HTMLæ–‡ä»¶æ‰€åœ¨ç›®å½• (é»˜è®¤: å½“å‰ç›®å½•)'
    )

    parser.add_argument(
        '-o', '--output',
        default='tweets_output.json',
        help='è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: tweets_output.json)'
    )

    parser.add_argument(
        '--single',
        help='åªå¤„ç†å•ä¸ªHTMLæ–‡ä»¶'
    )

    parser.add_argument(
        '--preview',
        action='store_true',
        help='é¢„è§ˆæ¨¡å¼ï¼šåªè§£æžä¸ç”ŸæˆæŽ¨æ–‡'
    )

    args = parser.parse_args()

    if args.single:
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        print(f"å¤„ç†å•ä¸ªæ–‡ä»¶: {args.single}")
        profile = parse_wiki_html(args.single)
        print("\n=== æå–çš„èµ„æ–™ ===")
        for key, value in asdict(profile).items():
            if value:
                print(f"  {key}: {value}")

        if not args.preview:
            print("\n=== ç”Ÿæˆçš„æŽ¨æ–‡ ===")
            tweets = generate_tweets_with_ai(profile)
            for i, tweet in enumerate(tweets, 1):
                print(f"\n[{i}] {tweet}")
    else:
        # å¤„ç†æ•´ä¸ªç›®å½•
        if args.preview:
            # é¢„è§ˆæ¨¡å¼
            wiki_path = Path(args.wiki_folder)
            html_files = list(wiki_path.rglob("*.html"))
            print(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶:\n")

            for html_file in html_files[:5]:  # åªé¢„è§ˆå‰5ä¸ª
                profile = parse_wiki_html(str(html_file))
                print(f"ðŸ“„ {html_file.name}")
                print(f"   æˆå‘˜: {profile.name_en} ({profile.name_jp})")
                if profile.birthday:
                    print(f"   ç”Ÿæ—¥: {profile.birthday}")
                print()

            if len(html_files) > 5:
                print(f"... è¿˜æœ‰ {len(html_files) - 5} ä¸ªæ–‡ä»¶")
        else:
            process_wiki_folder(args.wiki_folder, args.output)


if __name__ == '__main__':
    main()
